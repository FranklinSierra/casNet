{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AtLJxriDlXsl"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irY2vJRElXsX"
   },
   "source": [
    "# CycleGAN\n",
    "\n",
    "#### Directory structure:\n",
    "- **data/** contains the datasets.\n",
    "    - **data/&lt;dataset&gt;/{train_A, train_B}/** contains training images for classes A and B.\n",
    "    - **data/&lt;dataset&gt;/{test_A, test_B}/** contains testing images that are not used during training. These are useful to evaluate the generalization of the model to new data.\n",
    "- **images/** stores metadata and loss information of each CycleGAN run, as well as evaluation images.\n",
    "    - **images/meta_data.json** contains the settings of the run.\n",
    "    - **images/loss_output.csv** contains the various losses of the model, stored after every batch.\n",
    "    - **images/{train_A, train_B, test_A, test_B}** contains intermediate evaluation images for each epoch, illustrating generator performance.\n",
    "    - **images/tmp.png** shows example image translations from the current moment in training. This image updates in real time and can be used to see how the training converges.\n",
    "- **saved_models** stores the generator and discriminator models resulting from each run, which are saved every 20 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tomado de:\n",
    "https://github.com/brainhack101/IntroDL/blob/master/notebooks/2019/Eklund/CycleGAN.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!rm -rf __pycache__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GEyU05yplivE"
   },
   "source": [
    "!wget -nc https://raw.githubusercontent.com/brainhack101/IntroDL/master/notebooks/2019/Eklund/requirements-gpu.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EF-Dt-UMl8aV"
   },
   "source": [
    "!pip install -r requirements-gpu.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yWUFie0RlXsb",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Layer, Input, Dropout, Conv2D, Activation, add, UpSampling2D, Conv2DTranspose, Flatten\n",
    "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization, InputSpec\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.engine.topology import Network\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from progress.bar import Bar\n",
    "import datetime\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "#to install: \n",
    "#\n",
    "#pip install --upgrade pip\n",
    "#matplot, date time, no tocar version de tf, la version de keras 2.2.4\n",
    "#pandas, scipy\n",
    "#pip install scikit-learn, pip install scipy, pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1l6S9SEKlXsi"
   },
   "source": [
    "Additional functions are contained in the `helper_functions.py` file. These mostly include code for loading the data and saving the resutls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whlevvwXnWrh"
   },
   "source": [
    "!wget https://raw.githubusercontent.com/brainhack101/IntroDL/master/notebooks/2019/Eklund/helper_funcs.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "rrnYRF9-lXsj"
   },
   "outputs": [],
   "source": [
    "from helper_funcs import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DgAYTVNKlXsl"
   },
   "source": [
    "If you have multiple GPUs you can select a single one of them by setting the visible CUDA device to 0, 1, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ushjqquylXso"
   },
   "source": [
    "#### Load data\n",
    "\n",
    "The dataset used for the run is **data/&lt;`image_folder`&gt;**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "h9tN-H7UlXsp"
   },
   "outputs": [],
   "source": [
    "image_folder = 'polyps'\n",
    "data = load_data(subfolder=image_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xomIz34dlXss"
   },
   "source": [
    "### Model parameters\n",
    "\n",
    "This CycleGAN implementation allows a lot of freedom on both the training parameters and the network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "qcQJERzclXst"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape:  (512, 512, 1)\n",
      "(1188, 512, 512, 1)\n",
      "(1188, 512, 512, 1)\n"
     ]
    }
   ],
   "source": [
    "opt = {}\n",
    "\n",
    "# Data\n",
    "opt['channels'] = data[\"nr_of_channels\"]\n",
    "opt['img_shape'] = data[\"image_size\"] + (opt['channels'],)\n",
    "print('Image shape: ', opt['img_shape'])\n",
    "\n",
    "opt['A_train'] = data[\"trainA_images\"]\n",
    "opt['B_train'] = data[\"trainB_images\"]\n",
    "opt['A_test'] = data[\"testA_images\"]\n",
    "opt['B_test'] = data[\"testB_images\"]\n",
    "opt['testA_image_names'] = data[\"testA_image_names\"]\n",
    "opt['testB_image_names'] = data[\"testB_image_names\"]\n",
    "\n",
    "opt['trainA_image_names'] = data[\"trainA_image_names\"]\n",
    "opt['trainB_image_names'] = data[\"trainB_image_names\"]\n",
    "\n",
    "#opt['CTFOSCAL_images'] = data[\"CTFOSCAL_images\"]\n",
    "#opt['CTFOSCAL_image_names'] = data[\"CTFOSCAL_image_names\"]\n",
    "#opt['CTHEALTHY_images'] = data[\"CTHEALTHY_images\"]\n",
    "#opt['CTHEALTHY_image_names'] = data[\"CTHEALTHY_image_names\"]\n",
    "\n",
    "print(opt['A_train'].shape)\n",
    "print(opt['B_train'].shape)\n",
    "\n",
    "#print(opt['CTFOSCAL_images'].shape)\n",
    "#print(len(opt['CTFOSCAL_image_names']))\n",
    "#print(opt['CTHEALTHY_images'].shape)\n",
    "#print(len(opt['CTHEALTHY_image_names']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ze3MSgrslXsw"
   },
   "source": [
    "CylceGAN can be used both on paired and unpaired data. The `paired_data` setting affects the presentation of output images as explained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "mIXSEhU2lXsw"
   },
   "outputs": [],
   "source": [
    "#opt['paired_data'] = True\n",
    "opt['paired_data'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_4ga7qRdlXsy"
   },
   "source": [
    "#### Training parameters\n",
    "- `lambda_ABA` and `lambda_BAB` set the importance of the cycle consistency losses in relation to the adversarial loss `lambda_adversarial`\n",
    "- `learning_rate_D` and `learning_rate_G` are the learning rates for the discriminators and generators respectively.\n",
    "- `generator_iterations` and `discriminator_iterations` represent how many times the generators or discriminators will be trained on every batch of images. This is very useful to keep the training of both systems balanced. In this case the discriminators become successful faster than the generators, so we account for this by training the generators 3 times on every batch of images.\n",
    "- `synthetic_pool_size` sets the size of the image pool used for training the discriminators. The image pool has a certain probability of returning a synthetic image from previous iterations, thus forcing the discriminator to have a certain \"memory\". More information on this method can be found in [this paper](https://arxiv.org/abs/1612.07828).\n",
    "- `beta_1` and `beta_2` are paremeters of the [Adam](https://arxiv.org/abs/1412.6980) optimizers used on the generators and discriminators.\n",
    "- `batch_size` determines the number of images used for each update of the network weights. Due to the significant memory requirements of CycleGAN it is difficult to use a large batch size. For the small example dataset values between 1-30 may be possible.\n",
    "- `epochs` sets the number of training epochs. Each epoch goes through all the training images once. The number of epochs necessary to train a model is therefore dependent on both the number of training images available and the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "pRB3NWjIlXsz"
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "opt['lambda_ABA'] = 10.0  # Cyclic loss weight A_2_B\n",
    "opt['lambda_BAB'] = 10.0  # Cyclic loss weight B_2_A\n",
    "opt['lambda_adversarial'] = 1.0  # Weight for loss from discriminator guess on synthetic images\n",
    "opt['learning_rate_D'] = 2e-4\n",
    "opt['learning_rate_G'] = 2e-4\n",
    "opt['generator_iterations'] = 3  # Number of generator training iterations in each training loop\n",
    "opt['discriminator_iterations'] = 1  # Number of discriminator training iterations in each training loop\n",
    "opt['synthetic_pool_size'] = 50  # Size of image pools used for training the discriminators\n",
    "opt['beta_1'] = 0.5  # Adam parameter\n",
    "opt['beta_2'] = 0.999  # Adam parameter\n",
    "opt['batch_size'] = 10  # Number of images per batch\n",
    "opt['epochs'] = 10  # Choose multiples of 20 since the models are saved each 20th epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "cXX_5VLwlXs1"
   },
   "outputs": [],
   "source": [
    "# Output parameters\n",
    "opt['save_models'] = True  # Save or not the generator and discriminator models\n",
    "opt['save_training_img'] = True  # Save or not example training results or only tmp.png\n",
    "opt['save_training_img_interval'] = 1  # Number of epoch between saves of intermediate training results\n",
    "opt['self.tmp_img_update_frequency'] = 3  # Number of batches between updates of tmp.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OaVHf9TNlXs4"
   },
   "source": [
    "#### Architecture parameters\n",
    "- `use_instance_normalization` is supposed to allow the selection of instance normalization or batch normalization layes. At the moment only instance normalization is implemented, so this option does not do anything.\n",
    "- `use_dropout` and `use_bias` allows setting droupout layers in the generators and whether to use a bias term in the various convolutional layer in the genrators and discriminators.\n",
    "- `use_linear_decay` applies linear decay on the learning rates of the generators and discriminators,   `decay_epoch`\n",
    "- `use_patchgan` determines whether the discriminator evaluates the \"realness\" of images on a patch basis or on the whole. More information on PatchGAN can be found in [this paper](https://arxiv.org/abs/1611.07004).\n",
    "- `use_resize_convolution` provides two ways to perfrom the upsampling in the generator, with significant differences in the results. More information can be found in [this article](https://distill.pub/2016/deconv-checkerboard/). Each has its advantages, and we have managed to get successful result with both methods\n",
    "- `use_discriminator sigmoid` adds a sigmoid activation at the end of the discrimintator, forcing its output to the (0-1) range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "WVnsr6pSlXs5"
   },
   "outputs": [],
   "source": [
    "# Architecture parameters\n",
    "opt['use_instance_normalization'] = True  # Use instance normalization or batch normalization\n",
    "opt['use_dropout'] = False  # Dropout in residual blocks\n",
    "opt['use_bias'] = True  # Use bias\n",
    "opt['use_linear_decay'] = True  # Linear decay of learning rate, for both discriminators and generators\n",
    "opt['decay_epoch'] = 100  # The epoch where the linear decay of the learning rates start\n",
    "opt['use_patchgan'] = True  # PatchGAN - if false the discriminator learning rate should be decreased\n",
    "opt['use_resize_convolution'] = False  # Resize convolution - instead of transpose convolution in deconvolution layers (uk) - can reduce checkerboard artifacts but the blurring might affect the cycle-consistency\n",
    "opt['discriminator_sigmoid'] = True  # Add a final sigmoid activation to the discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "PfOub3aQlXs7"
   },
   "outputs": [],
   "source": [
    "# Tweaks\n",
    "opt['REAL_LABEL'] = 1.0  # Use e.g. 0.9 to avoid training the discriminators to zero loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82fBbXHYlXs9"
   },
   "source": [
    "### Model architecture\n",
    "\n",
    "#### Layer blocks\n",
    "These are the individual layer blocks that are used to build the generators and discriminator. More information can be found in the appendix of the [CycleGAN paper](https://arxiv.org/abs/1703.10593)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "hQuI04BwlXs-"
   },
   "outputs": [],
   "source": [
    "# Discriminator layers\n",
    "def ck(model, opt, x, k, use_normalization, use_bias):\n",
    "    x = Conv2D(filters=k, kernel_size=4, strides=2, padding='same', use_bias=use_bias)(x)\n",
    "    if use_normalization:\n",
    "        x = model['normalization'](axis=3, center=True, epsilon=1e-5)(x, training=True)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    return x\n",
    "\n",
    "# First generator layer\n",
    "def c7Ak(model, opt, x, k):\n",
    "    x = Conv2D(filters=k, kernel_size=7, strides=1, padding='valid', use_bias=opt['use_bias'])(x)\n",
    "    x = model['normalization'](axis=3, center=True, epsilon=1e-5)(x, training=True)\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "# Downsampling\n",
    "def dk(model, opt, x, k):  # Should have reflection padding\n",
    "    x = Conv2D(filters=k, kernel_size=3, strides=2, padding='same', use_bias=opt['use_bias'])(x)\n",
    "    x = model['normalization'](axis=3, center=True, epsilon=1e-5)(x, training=True)\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "# Residual block\n",
    "def Rk(model, opt, x0):\n",
    "    k = int(x0.shape[-1])\n",
    "\n",
    "    # First layer\n",
    "    x = ReflectionPadding2D((1,1))(x0)\n",
    "    x = Conv2D(filters=k, kernel_size=3, strides=1, padding='valid', use_bias=opt['use_bias'])(x)\n",
    "    x = model['normalization'](axis=3, center=True, epsilon=1e-5)(x, training=True)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    if opt['use_dropout']:\n",
    "        x = Dropout(0.5)(x)\n",
    "\n",
    "    # Second layer\n",
    "    x = ReflectionPadding2D((1, 1))(x)\n",
    "    x = Conv2D(filters=k, kernel_size=3, strides=1, padding='valid', use_bias=opt['use_bias'])(x)\n",
    "    x = model['normalization'](axis=3, center=True, epsilon=1e-5)(x, training=True)\n",
    "    # Merge\n",
    "    x = add([x, x0])\n",
    "\n",
    "    return x\n",
    "\n",
    "# Upsampling\n",
    "def uk(model, opt, x, k):\n",
    "    # (up sampling followed by 1x1 convolution <=> fractional-strided 1/2)\n",
    "    if opt['use_resize_convolution']:\n",
    "        x = UpSampling2D(size=(2, 2))(x)  # Nearest neighbor upsampling\n",
    "        x = ReflectionPadding2D((1, 1))(x)\n",
    "        x = Conv2D(filters=k, kernel_size=3, strides=1, padding='valid', use_bias=opt['use_bias'])(x)\n",
    "    else:\n",
    "        x = Conv2DTranspose(filters=k, kernel_size=3, strides=2, padding='same', use_bias=opt['use_bias'])(x)  # this matches fractionally stided with stride 1/2\n",
    "    x = model['normalization'](axis=3, center=True, epsilon=1e-5)(x, training=True)\n",
    "    x = Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejV66kdmlXtA"
   },
   "source": [
    "#### Architecture functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "gDicsY-PlXtD"
   },
   "outputs": [],
   "source": [
    "def build_discriminator(model, opt, name=None):\n",
    "    # Input\n",
    "    input_img = Input(shape=opt['img_shape'])\n",
    "\n",
    "    # Layers 1-4\n",
    "    x = ck(model, opt, input_img, 64, False, True) #  Instance normalization is not used for this layer)\n",
    "    x = ck(model, opt, x, 128, True, opt['use_bias'])\n",
    "    x = ck(model, opt, x, 256, True, opt['use_bias'])\n",
    "    x = ck(model, opt, x, 512, True, opt['use_bias'])\n",
    "\n",
    "    # Layer 5: Output\n",
    "    if opt['use_patchgan']:\n",
    "        x = Conv2D(filters=1, kernel_size=4, strides=1, padding='same', use_bias=True)(x)\n",
    "    else:\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(1)(x)\n",
    "\n",
    "    if opt['discriminator_sigmoid']:\n",
    "        x = Activation('sigmoid')(x)\n",
    "\n",
    "    return Model(inputs=input_img, outputs=x, name=name)\n",
    "\n",
    "def build_generator(model, opt, name=None):\n",
    "    # Layer 1: Input\n",
    "    input_img = Input(shape=opt['img_shape'])\n",
    "    x = ReflectionPadding2D((3, 3))(input_img)\n",
    "    x = c7Ak(model, opt, x, 32)\n",
    "\n",
    "    # Layer 2-3: Downsampling\n",
    "    x = dk(model, opt, x, 64)\n",
    "    x = dk(model, opt, x, 128)\n",
    "\n",
    "    # Layers 4-12: Residual blocks\n",
    "    for _ in range(4, 13):\n",
    "        x = Rk(model, opt, x)\n",
    "\n",
    "    # Layer 13:14: Upsampling\n",
    "    x = uk(model, opt, x, 64)\n",
    "    x = uk(model, opt, x, 32)\n",
    "\n",
    "    # Layer 15: Output\n",
    "    x = ReflectionPadding2D((3, 3))(x)\n",
    "    x = Conv2D(opt['channels'], kernel_size=7, strides=1, padding='valid', use_bias=True)(x)\n",
    "    x = Activation('tanh')(x)\n",
    "\n",
    "    return Model(inputs=input_img, outputs=x, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CasNet generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Generator\n",
    "from keras import layers\n",
    "\n",
    "def u_block(input_tensor, \n",
    "            encoder_num_filters=[64, 128, 256, 512, 512, 512, 512, 512],\n",
    "            decoder_num_filters=[512, 1024, 1024, 1024, 1024, 512, 256, 128],\n",
    "            use_instance_norm=True,\n",
    "            block_prefix=''):\n",
    "    \n",
    "    x = input_tensor\n",
    "    encoder_blocks_outputs = []\n",
    "    \n",
    "    # Encoder path.\n",
    "    for i, num_filters in enumerate(encoder_num_filters):\n",
    "        x = layers.Conv2D(num_filters, \n",
    "                          kernel_size=4, \n",
    "                          strides=2, \n",
    "                          padding='same', \n",
    "                          name=f'{block_prefix}_EncoderBlock{i+1}-Conv')(x)\n",
    "        if use_instance_norm:\n",
    "            x = InstanceNormalization(name=f'{block_prefix}_EncoderBlock{i+1}-Instancenorm')(x)\n",
    "        else:\n",
    "            x = layers.BatchNormalization(name=f'{block_prefix}_EncoderBlock{i+1}-Batchnorm')(x)\n",
    "        x = layers.Activation('relu', name=f'{block_prefix}_EncoderBlock{i+1}-ReLU')(x)\n",
    "        \n",
    "        # Append the encoder blocks outputs, except for the last one.\n",
    "        if i != len(encoder_num_filters) - 1: \n",
    "            encoder_blocks_outputs.append(x)\n",
    "\n",
    "    # Decoder path.\n",
    "    for i, num_filters in enumerate(decoder_num_filters):\n",
    "        x = layers.Conv2DTranspose(\n",
    "            num_filters, \n",
    "            kernel_size=(4, 4), \n",
    "            strides=(2, 2), \n",
    "            padding='same',\n",
    "            name=f'{block_prefix}_DecoderBlock{i+1}-TransposedConv'\n",
    "        )(x)\n",
    "        x = layers.Activation('relu', name=f'{block_prefix}_DecoderBlock{i+1}-ReLU')(x)\n",
    "\n",
    "        # All the decoder blocks have concatenate the encoder block output with\n",
    "        # the same spatial dimentions except for the last one.\n",
    "        if i != len(decoder_num_filters) - 1: \n",
    "            x = layers.Concatenate(\n",
    "                name=f'{block_prefix}_DecoderBlock{i+1}-Concat'\n",
    "            )([encoder_blocks_outputs[-(i + 1)], x])\n",
    "\n",
    "    # Last operation: 1x1 conv to map the image to the input n_channels.\n",
    "    x = layers.Conv2D(1,   #input_tensor.shape[-1]\n",
    "                      kernel_size=(1, 1), \n",
    "                      name=f'{block_prefix}_Decoder-Conv1x1')(x)\n",
    "    output = layers.Activation('tanh', name=f'{block_prefix}_Decoder-Tanh')(x)\n",
    "\n",
    "    return output\n",
    "#input_shape=(256, 256, 3)cambio posible para frank\n",
    "def build_casnet_generator(input_shape=(256, 256, 1), \n",
    "                           n_blocks=2, \n",
    "                           use_instance_norm=True,\n",
    "                           name=None):\n",
    "    \"\"\"Create a CasNet Generator using UBlocks\"\"\"\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "    x = input_tensor\n",
    "    for i in range(n_blocks):\n",
    "        x = u_block(x, block_prefix=f'UBlock{i+1}', use_instance_norm=use_instance_norm)\n",
    "    \n",
    "    return Model(inputs=input_tensor, outputs=x, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytd1YAMXlXtF"
   },
   "source": [
    "#### Loss functions\n",
    "The discriminators use MSE loss. The generators use MSE for the adversarial losses and MAE for the cycle consistency losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "NhESrpK9lXtF"
   },
   "outputs": [],
   "source": [
    "# Mean squared error\n",
    "def mse(y_true, y_pred):\n",
    "    loss = tf.reduce_mean(tf.squared_difference(y_pred, y_true))\n",
    "    return loss\n",
    "\n",
    "# Mean absolute error\n",
    "def mae(y_true, y_pred):\n",
    "    loss = tf.reduce_mean(tf.abs(y_pred - y_true))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7B6aXKllXtI"
   },
   "source": [
    "#### Build CycleGAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "sBLv_l3llXtJ",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0127 22:25:18.397768 139719884687168 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0127 22:25:18.398958 139719884687168 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0127 22:25:18.402125 139719884687168 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0127 22:25:18.818581 139719884687168 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 512, 512, 1)\n"
     ]
    }
   ],
   "source": [
    "model = {}\n",
    "\n",
    "# Normalization\n",
    "model['normalization'] = InstanceNormalization\n",
    "\n",
    "# Optimizers\n",
    "model['opt_D'] = Adam(opt['learning_rate_D'], opt['beta_1'], opt['beta_2'])\n",
    "model['opt_G'] = Adam(opt['learning_rate_G'], opt['beta_1'], opt['beta_2'])\n",
    "\n",
    "# Build discriminators\n",
    "D_A = build_discriminator(model, opt, name='D_A')\n",
    "D_B = build_discriminator(model, opt, name='D_B')\n",
    "\n",
    "# Define discriminator models\n",
    "image_A = Input(shape=opt['img_shape'])\n",
    "print(image_A.shape)\n",
    "image_B = Input(shape=opt['img_shape'])\n",
    "guess_A = D_A(image_A)\n",
    "guess_B = D_B(image_B)\n",
    "model['D_A'] = Model(inputs=image_A, outputs=guess_A, name='D_A_model')\n",
    "model['D_B'] = Model(inputs=image_B, outputs=guess_B, name='D_B_model')\n",
    "\n",
    "# Compile discriminator models\n",
    "loss_weights_D = [0.5]  # 0.5 since we train on real and synthetic images\n",
    "model['D_A'].compile(optimizer=model['opt_D'],\n",
    "                 loss=mse,\n",
    "                 loss_weights=loss_weights_D)\n",
    "model['D_B'].compile(optimizer=model['opt_D'],\n",
    "                 loss=mse,\n",
    "                 loss_weights=loss_weights_D)\n",
    "\n",
    "# Use containers to make a static copy of discriminators, used when training the generators\n",
    "model['D_A_static'] = Network(inputs=image_A, outputs=guess_A, name='D_A_static_model')\n",
    "model['D_B_static'] = Network(inputs=image_B, outputs=guess_B, name='D_B_static_model')\n",
    "\n",
    "# Do not update discriminator weights during generator training\n",
    "model['D_A_static'].trainable = False\n",
    "model['D_B_static'].trainable = False\n",
    "\n",
    "# Build generators\n",
    "#model['G_A2B'] = build_generator(model, opt, name='G_A2B_model')\n",
    "#model['G_B2A'] = build_generator(model, opt, name='G_B2A_model')\n",
    "\n",
    "#input_shape=(512,512,3) cambio posible para frank \n",
    "model['G_A2B'] = build_casnet_generator(input_shape=(512,512,1), n_blocks=1, name='G_A2B_model')\n",
    "model['G_B2A'] = build_casnet_generator(input_shape=(512,512,1), n_blocks=1, name='G_B2A_model')\n",
    "\n",
    "#print(\"G_A2B\")\n",
    "#model['G_A2B'].summary()\n",
    "#print(\"G_B2A\")\n",
    "#model['G_B2A'].summary()\n",
    "\n",
    "# Define full CycleGAN model, used for training the generators\n",
    "real_A = Input(shape=opt['img_shape'], name='real_A')\n",
    "real_B = Input(shape=opt['img_shape'], name='real_B')\n",
    "synthetic_B = model['G_A2B'](real_A)\n",
    "synthetic_A = model['G_B2A'](real_B)\n",
    "dB_guess_synthetic = model['D_B_static'](synthetic_B)\n",
    "dA_guess_synthetic = model['D_A_static'](synthetic_A)\n",
    "reconstructed_A = model['G_B2A'](synthetic_B)\n",
    "reconstructed_B = model['G_A2B'](synthetic_A)\n",
    "\n",
    "# Compile full CycleGAN model\n",
    "model_outputs = [reconstructed_A, reconstructed_B,\n",
    "                 dB_guess_synthetic, dA_guess_synthetic]\n",
    "compile_losses = [mae, mae,\n",
    "                  mse, mse]\n",
    "compile_weights = [opt['lambda_ABA'], opt['lambda_BAB'],\n",
    "                   opt['lambda_adversarial'], opt['lambda_adversarial']]\n",
    "\n",
    "model['G_model'] = Model(inputs=[real_A, real_B],\n",
    "                     outputs=model_outputs,\n",
    "                     name='G_model')\n",
    "\n",
    "model['G_model'].compile(optimizer=model['opt_G'],\n",
    "                     loss=compile_losses,\n",
    "                     loss_weights=compile_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPgzq3VjlXtM"
   },
   "source": [
    "#### Folders and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "y3yDA_O7lXtN"
   },
   "outputs": [],
   "source": [
    "opt['date_time'] = time.strftime('%Y%m%d-%H%M%S', time.localtime()) + '-' + image_folder\n",
    "\n",
    "# Output folder for run data and images\n",
    "opt['out_dir'] = os.path.join('images', opt['date_time'])\n",
    "if not os.path.exists(opt['out_dir']):\n",
    "    os.makedirs(opt['out_dir'])\n",
    "\n",
    "# Output folder for saved models\n",
    "if opt['save_models']:\n",
    "    opt['model_out_dir'] = os.path.join('saved_models', opt['date_time'])\n",
    "    if not os.path.exists(opt['model_out_dir']):\n",
    "        os.makedirs(opt['model_out_dir'])\n",
    "\n",
    "write_metadata_to_JSON(model, opt)\n",
    "\n",
    "# Don't pre-allocate GPU memory; allocate as-needed\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "K.tensorflow_backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nk04cLeklXtQ"
   },
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "4Gi4VtqjlXtS"
   },
   "outputs": [],
   "source": [
    "def train(model, opt):\n",
    "\n",
    "    def run_training_batch():\n",
    "\n",
    "        # ======= Discriminator training ======\n",
    "        # Generate batch of synthetic images\n",
    "        synthetic_images_B = model['G_A2B'].predict(real_images_A)\n",
    "        synthetic_images_A = model['G_B2A'].predict(real_images_B)\n",
    "        synthetic_images_B = synthetic_pool_B.query(synthetic_images_B)\n",
    "        synthetic_images_A = synthetic_pool_A.query(synthetic_images_A)\n",
    "\n",
    "        # Train discriminators on batch\n",
    "        D_loss = []\n",
    "        for _ in range(opt['discriminator_iterations']):\n",
    "            D_A_loss_real = model['D_A'].train_on_batch(x=real_images_A, y=ones)\n",
    "            D_B_loss_real = model['D_B'].train_on_batch(x=real_images_B, y=ones)\n",
    "            D_A_loss_synthetic = model['D_A'].train_on_batch(x=synthetic_images_A, y=zeros)\n",
    "            D_B_loss_synthetic = model['D_B'].train_on_batch(x=synthetic_images_B, y=zeros)\n",
    "            D_A_loss = D_A_loss_real + D_A_loss_synthetic\n",
    "            D_B_loss = D_B_loss_real + D_B_loss_synthetic\n",
    "            D_loss.append(D_A_loss + D_B_loss)\n",
    "\n",
    "        # ======= Generator training ==========\n",
    "        target_data = [real_images_A, real_images_B, ones, ones]  # Reconstructed images need to match originals, discriminators need to predict ones\n",
    "\n",
    "        # Train generators on batch\n",
    "        G_loss = []\n",
    "        for _ in range(opt['generator_iterations']):\n",
    "            G_loss.append(model['G_model'].train_on_batch(\n",
    "                x=[real_images_A, real_images_B], y=target_data))\n",
    "\n",
    "        # =====================================\n",
    "\n",
    "        # Update learning rates\n",
    "        if opt['use_linear_decay'] and epoch >= opt['decay_epoch']:\n",
    "            update_lr(model['D_A'], decay_D)\n",
    "            update_lr(model['D_B'], decay_D)\n",
    "            update_lr(model['G_model'], decay_G)\n",
    "\n",
    "        # Store training losses\n",
    "        D_A_losses.append(D_A_loss)\n",
    "        D_B_losses.append(D_B_loss)\n",
    "        D_losses.append(D_loss[-1])\n",
    "\n",
    "        ABA_reconstruction_loss = G_loss[-1][1]\n",
    "        BAB_reconstruction_loss = G_loss[-1][2]\n",
    "        reconstruction_loss = ABA_reconstruction_loss + BAB_reconstruction_loss\n",
    "        G_AB_adversarial_loss = G_loss[-1][3]\n",
    "        G_BA_adversarial_loss = G_loss[-1][4]\n",
    "\n",
    "        ABA_reconstruction_losses.append(ABA_reconstruction_loss)\n",
    "        BAB_reconstruction_losses.append(BAB_reconstruction_loss)\n",
    "        reconstruction_losses.append(reconstruction_loss)\n",
    "        G_AB_adversarial_losses.append(G_AB_adversarial_loss)\n",
    "        G_BA_adversarial_losses.append(G_BA_adversarial_loss)\n",
    "        G_losses.append(G_loss[-1][0])\n",
    "\n",
    "        # Print training status\n",
    "        print('\\n')\n",
    "        print('Epoch ---------------------', epoch, '/', opt['epochs'])\n",
    "        print('Loop index ----------------', loop_index + 1, '/', nr_im_per_epoch)\n",
    "        if opt['discriminator_iterations'] > 1:\n",
    "            print('  Discriminator losses:')\n",
    "            for i in range(opt['discriminator_iterations']):\n",
    "                print('D_loss', D_loss[i])\n",
    "        if opt['generator_iterations'] > 1:\n",
    "            print('  Generator losses:')\n",
    "            for i in range(opt['generator_iterations']):\n",
    "                print('G_loss', G_loss[i])\n",
    "        print('  Summary:')\n",
    "        print('D_lr:', K.get_value(model['D_A'].optimizer.lr))\n",
    "        print('G_lr', K.get_value(model['G_model'].optimizer.lr))\n",
    "        print('D_loss: ', D_loss[-1])\n",
    "        print('G_loss: ', G_loss[-1][0])\n",
    "        print('reconstruction_loss: ', reconstruction_loss)\n",
    "        print_ETA(opt, start_time, epoch, nr_im_per_epoch, loop_index)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        if loop_index % 3*opt['batch_size'] == 0:\n",
    "            # Save temporary images continously\n",
    "            save_tmp_images(model, opt, real_images_A[0], real_images_B[0],\n",
    "                                 synthetic_images_A[0], synthetic_images_B[0])\n",
    "\n",
    "    # ======================================================================\n",
    "    # Begin training\n",
    "    # ======================================================================\n",
    "    if opt['save_training_img'] and not os.path.exists(os.path.join(opt['out_dir'], 'train_A')):\n",
    "        os.makedirs(os.path.join(opt['out_dir'], 'train_A'))\n",
    "        os.makedirs(os.path.join(opt['out_dir'], 'train_B'))\n",
    "        os.makedirs(os.path.join(opt['out_dir'], 'test_A'))\n",
    "        os.makedirs(os.path.join(opt['out_dir'], 'test_B'))\n",
    "\n",
    "    D_A_losses = []\n",
    "    D_B_losses = []\n",
    "    D_losses = []\n",
    "\n",
    "    ABA_reconstruction_losses = []\n",
    "    BAB_reconstruction_losses = []\n",
    "    reconstruction_losses = []\n",
    "    G_AB_adversarial_losses = []\n",
    "    G_BA_adversarial_losses = []\n",
    "    G_losses = []\n",
    "\n",
    "    # Image pools used to update the discriminators\n",
    "    synthetic_pool_A = ImagePool(opt['synthetic_pool_size'])\n",
    "    synthetic_pool_B = ImagePool(opt['synthetic_pool_size'])\n",
    "\n",
    "    # Labels used for discriminator training\n",
    "    label_shape = (opt['batch_size'],) + model['D_A'].output_shape[1:]\n",
    "    ones = np.ones(shape=label_shape) * opt['REAL_LABEL']\n",
    "    zeros = ones * 0\n",
    "\n",
    "    # Linear learning rate decay\n",
    "    if opt['use_linear_decay']:\n",
    "        decay_D, decay_G = get_lr_linear_decay_rate(opt)\n",
    "\n",
    "    nr_train_im_A = opt['A_train'].shape[0]\n",
    "    nr_train_im_B = opt['B_train'].shape[0]\n",
    "    nr_im_per_epoch = int(np.ceil(np.max((nr_train_im_A, nr_train_im_B)) / opt['batch_size']) * opt['batch_size'])\n",
    "\n",
    "    # Start stopwatch for ETAs\n",
    "    start_time = time.time()\n",
    "    timer_started = False\n",
    "\n",
    "    for epoch in range(1, opt['epochs'] + 1):\n",
    "        # random_order_A = np.random.randint(nr_train_im_A, size=nr_im_per_epoch)\n",
    "        # random_order_B = np.random.randint(nr_train_im_B, size=nr_im_per_epoch)\n",
    "\n",
    "        random_order_A = np.concatenate((np.random.permutation(nr_train_im_A),\n",
    "                                         np.random.randint(nr_train_im_A, size=nr_im_per_epoch - nr_train_im_A)))\n",
    "        random_order_B = np.concatenate((np.random.permutation(nr_train_im_B),\n",
    "                                         np.random.randint(nr_train_im_B, size=nr_im_per_epoch - nr_train_im_B)))\n",
    "\n",
    "        # Train on image batch\n",
    "        for loop_index in range(0, nr_im_per_epoch, opt['batch_size']):\n",
    "            indices_A = random_order_A[loop_index:loop_index + opt['batch_size']]\n",
    "            indices_B = random_order_B[loop_index:loop_index + opt['batch_size']]\n",
    "\n",
    "            real_images_A = opt['A_train'][indices_A]\n",
    "            real_images_B = opt['B_train'][indices_B]\n",
    "\n",
    "            # Train on image batch\n",
    "            run_training_batch()\n",
    "\n",
    "            # Start timer after first (slow) iteration has finished\n",
    "            if not timer_started:\n",
    "                start_time = time.time()\n",
    "                timer_started = True\n",
    "\n",
    "        # Save training images\n",
    "        if opt['save_training_img'] and epoch % opt['save_training_img_interval'] == 0:\n",
    "            print('\\n', '\\n', '-------------------------Saving images for epoch', epoch, '-------------------------', '\\n', '\\n')\n",
    "            save_epoch_images(model, opt, epoch)\n",
    "\n",
    "        # Save model\n",
    "        if opt['save_models'] and epoch % 1 == 0:\n",
    "            save_model(opt, model['D_A'], epoch)\n",
    "            save_model(opt, model['D_B'], epoch)\n",
    "            save_model(opt, model['G_A2B'], epoch)\n",
    "            save_model(opt, model['G_B2A'], epoch)\n",
    "\n",
    "        # Save training history\n",
    "        training_history = {\n",
    "            'DA_losses': D_A_losses,\n",
    "            'DB_losses': D_B_losses,\n",
    "            'G_AB_adversarial_losses': G_AB_adversarial_losses,\n",
    "            'G_BA_adversarial_losses': G_BA_adversarial_losses,\n",
    "            'ABA_reconstruction_losses': ABA_reconstruction_losses,\n",
    "            'BAB_reconstruction_losses': BAB_reconstruction_losses,\n",
    "            'reconstruction_losses': reconstruction_losses,\n",
    "            'D_losses': D_losses,\n",
    "            'G_losses': G_losses}\n",
    "        write_loss_data_to_file(opt, training_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7G-fVM6flXtW"
   },
   "source": [
    "### Train CycleGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2_0XjcQ3lXtW",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(model, opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HoWx9DDylXtZ"
   },
   "source": [
    "### Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folder = \"20211209-235929-brain\" #2ublocks\n",
    "#folder = \"20211213-234440-brain\" #1ublocks\n",
    "#folder = \"20211213-234440-brain\" #1ublocks 6epochs ---> embebidos\n",
    "#folder = \"20211214-223746-brain\" #2ublocks 2epochs\n",
    "#folder = \"20211214-232746-brain\" #2ublocks 10epochs ---> embebidos\n",
    "#folder = \"20211215-141924-brain\" #2ublocks 4epochs 4g1d\n",
    "#folder = \"20211215-175957-brain\" #2ublocks 4epochs 3g2d\n",
    "\n",
    "folder = \"20220112-215315-brain\" #1ublocks 10epochs (mejor valor con 5 epochs)\n",
    "\n",
    "#folder = \"20220119-154149-brain\" #BORRAR 2ublocks 10 epochs\n",
    "\n",
    "ep = 1\n",
    "checkpoint_path1 = \"saved_models/\"+folder+\"/G_A2B_model_weights_epoch_\"+str(ep)+\".hdf5\"\n",
    "checkpoint_path2 = \"saved_models/\"+folder+\"/G_B2A_model_weights_epoch_\"+str(ep)+\".hdf5\"\n",
    "model[\"G_A2B\"].load_weights(checkpoint_path1)\n",
    "model[\"G_B2A\"].load_weights(checkpoint_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_A = opt['A_train'][1]\n",
    "test_images_B = opt['B_train'][1]\n",
    "print(opt['A_train'].shape)\n",
    "print(opt['B_train'].shape)\n",
    "\n",
    "synthetic_images_B = model['G_A2B'].predict(test_images_A[np.newaxis,:])\n",
    "reconstructed_image_A = model['G_B2A'].predict(synthetic_images_B)\n",
    "\n",
    "synthetic_images_A = model['G_B2A'].predict(test_images_B[np.newaxis,:])\n",
    "reconstructed_image_B = model['G_A2B'].predict(synthetic_images_A)\n",
    "\n",
    "plt.figure(figsize=(15,20))\n",
    "plt.subplot(4,3,1)\n",
    "plt.imshow(test_images_A[:,:,0], cmap=\"gray\")\n",
    "plt.subplot(4,3,2)\n",
    "plt.imshow(synthetic_images_B[0,:,:,0], cmap=\"gray\")\n",
    "plt.subplot(4,3,3)\n",
    "plt.imshow(reconstructed_image_A[0,:,:,0], cmap=\"gray\")\n",
    "\n",
    "plt.subplot(4,3,4)\n",
    "plt.imshow(test_images_B[:,:,0], cmap=\"gray\")\n",
    "plt.subplot(4,3,5)\n",
    "plt.imshow(synthetic_images_A[0,:,:,0], cmap=\"gray\")\n",
    "plt.subplot(4,3,6)\n",
    "plt.imshow(reconstructed_image_B[0,:,:,0], cmap=\"gray\")\n",
    "\n",
    "\n",
    "CTHEALTHY_images_A = opt['CTHEALTHY_images'][1]\n",
    "\n",
    "synthetic_images_B = model['G_A2B'].predict(CTHEALTHY_images_A[np.newaxis,:])\n",
    "reconstructed_image_A = model['G_B2A'].predict(synthetic_images_B)\n",
    "\n",
    "plt.subplot(4,3,7)\n",
    "plt.imshow(CTHEALTHY_images_A[:,:,0], cmap=\"gray\")\n",
    "plt.subplot(4,3,8)\n",
    "plt.imshow(synthetic_images_B[0,:,:,0], cmap=\"gray\")\n",
    "plt.subplot(4,3,9)\n",
    "plt.imshow(reconstructed_image_A[0,:,:,0], cmap=\"gray\")\n",
    "\n",
    "\n",
    "CTFOSCAL_images_A = opt['CTFOSCAL_images'][10]\n",
    "\n",
    "synthetic_images_B = model['G_A2B'].predict(CTFOSCAL_images_A[np.newaxis,:])\n",
    "reconstructed_image_A = model['G_B2A'].predict(synthetic_images_B)\n",
    "\n",
    "plt.subplot(4,3,10)\n",
    "plt.imshow(CTFOSCAL_images_A[:,:,0], cmap=\"gray\")\n",
    "plt.subplot(4,3,11)\n",
    "plt.imshow(synthetic_images_B[0,:,:,0], cmap=\"gray\")\n",
    "plt.subplot(4,3,12)\n",
    "plt.imshow(reconstructed_image_A[0,:,:,0], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic MRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "capa_o = keras.models.Model(inputs=model[\"G_A2B\"].get_input_at(0), outputs=model[\"G_A2B\"].get_layer('UBlock1_EncoderBlock8-ReLU').output)\n",
    "capa_o2 = keras.models.Model(inputs=model[\"G_B2A\"].get_input_at(0), outputs=model[\"G_B2A\"].get_layer('UBlock1_EncoderBlock8-ReLU').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "synthetic_images_B = model['G_A2B'].predict(opt['A_train'])\n",
    "\n",
    "synthetic_images_B_CTHEALTHY = model['G_A2B'].predict(opt['CTHEALTHY_images'])\n",
    "synthetic_images_B_CTFOSCAL = model['G_A2B'].predict(opt['CTFOSCAL_images'])\n",
    "\n",
    "print(opt['trainA_image_names'][0])\n",
    "print(opt['CTHEALTHY_image_names'][0])\n",
    "print(opt['CTFOSCAL_image_names'][0])\n",
    "\n",
    "emb = capa_o.predict(synthetic_images_B)\n",
    "print(emb[:,0,0,:].shape, emb.shape)\n",
    "\n",
    "emb2 = capa_o2.predict(opt['B_train'])\n",
    "print(emb2[:,0,0,:].shape)\n",
    "\n",
    "emb3 = capa_o2.predict(synthetic_images_B_CTHEALTHY)\n",
    "print(emb3[:,0,0,:].shape)\n",
    "\n",
    "emb4 = capa_o2.predict(synthetic_images_B_CTFOSCAL)\n",
    "print(emb4[:,0,0,:].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi^2 metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "d = pd.read_csv(\"data/xxx2.csv\")\n",
    "\n",
    "#mb_list = []\n",
    "mb_synth_ns = []\n",
    "mb_synth_stroke = []\n",
    "\n",
    "for i in range(len(opt['trainA_image_names'])):\n",
    "    #mb_list.append(list(d.loc[d[\"CT\"] == opt['trainA_image_names'][i]][\"maskbin\"])[0])\n",
    "    if (list(d.loc[d[\"CT\"] == opt['trainA_image_names'][i]][\"maskbin\"])[0]==0):\n",
    "        mb_synth_ns.append(emb2[i,0,0,:])\n",
    "    else:\n",
    "        mb_synth_stroke.append(emb2[i,0,0,:])\n",
    "\n",
    "mb_synth_ns = np.array(mb_synth_ns)\n",
    "mb_synth_stroke = np.array(mb_synth_stroke)\n",
    "\n",
    "print(mb_synth_ns.shape)\n",
    "print(mb_synth_stroke.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chisquare\n",
    "\n",
    "a = chisquare(mb_synth_ns, mb_synth_stroke[:196])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "comp = pca.fit_transform(emb[:,0,0,:])\n",
    "\n",
    "pca2 = PCA(n_components=2)\n",
    "comp2 = pca2.fit_transform(emb2[:,0,0,:])\n",
    "\n",
    "pca3 = PCA(n_components=2)\n",
    "comp3 = pca3.fit_transform(emb3[:,0,0,:])\n",
    "\n",
    "pca4 = PCA(n_components=2)\n",
    "comp4 = pca4.fit_transform(emb4[:,0,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(comp[:,0], comp[:,1], label=\"Synthetic MRI\", alpha=0.5)\n",
    "plt.scatter(comp2[:,0], comp2[:,1], label=\"MRI_train\", alpha=0.3)\n",
    "plt.scatter(comp3[:,0], comp3[:,1], label=\"Synthetic MRI CTHEALTHY\", alpha=0.1)\n",
    "plt.scatter(comp4[:,0], comp4[:,1], label=\"Synthetic MRI CTFOSCAL\", alpha=0.1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "print(emb.shape)\n",
    "print(emb2.shape)\n",
    "um = umap.UMAP().fit_transform(emb[:,0,0,:])\n",
    "um2 = umap.UMAP().fit_transform(emb2[:,0,0,:])\n",
    "um3 = umap.UMAP().fit_transform(emb3[:,0,0,:])\n",
    "um4 = umap.UMAP().fit_transform(emb4[:,0,0,:])\n",
    "\n",
    "plt.scatter(um[:,0], um[:,1], label=\"Synthetic MRI\", alpha=0.5)\n",
    "plt.scatter(um2[:,0], um2[:,1], label=\"MRI_train\", alpha=0.5)\n",
    "plt.scatter(um3[:,0], um3[:,1], label=\"Synthetic MRI CTHEALTHY\", alpha=0.5)\n",
    "plt.scatter(um4[:,0], um4[:,1], label=\"Synthetic MRI CTFOSCAL\", alpha=0.5)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "d = pd.read_csv(\"data/xxx2.csv\")\n",
    "\n",
    "print(opt['trainA_image_names'][0])\n",
    "\n",
    "mb_list = []\n",
    "mb_synth_ns = []\n",
    "mb_synth_stroke = []\n",
    "\n",
    "print(type(comp))\n",
    "for i in range(len(opt['trainA_image_names'])):\n",
    "    #fname, comp[i,0], comp[i,1], maskbin\n",
    "    mb_list.append(list(d.loc[d[\"CT\"] == opt['trainA_image_names'][i]][\"maskbin\"])[0])\n",
    "    if (list(d.loc[d[\"CT\"] == opt['trainA_image_names'][i]][\"maskbin\"])[0]==0):\n",
    "        mb_synth_ns.append([comp[i,0], comp[i,1]])\n",
    "    else:\n",
    "        mb_synth_stroke.append([comp[i,0], comp[i,1]])\n",
    "\n",
    "mb_synth_ns = np.array(mb_synth_ns)\n",
    "mb_synth_stroke = np.array(mb_synth_stroke)\n",
    "\n",
    "\n",
    "#CENTROIDES\n",
    "c_stroke = np.mean(mb_synth_stroke, axis=0)\n",
    "c_ns = np.mean(mb_synth_ns, axis=0)\n",
    "c_h = np.mean(comp3, axis=0)\n",
    "print(c_stroke)\n",
    "print(c_ns)\n",
    "print(c_h)\n",
    "#for i in mb_synth_stroke.shape[0]:\n",
    "#    mb_synth_stroke[:,0]\n",
    "\n",
    "\n",
    "#d = {'files': opt[\"trainA_image_names\"], 'maskbin': mb_list, 'x_umap': um[:,0], 'y_umap': um[:,1], 'x_pca': comp[:,0], 'y_pca': comp[:,1]}\n",
    "d = {'files': opt[\"trainA_image_names\"], 'maskbin': mb_list, 'x_pca': comp[:,0], 'y_pca': comp[:,1]}\n",
    "d = pd.DataFrame(data=d)\n",
    "\n",
    "#sns.set(rc={'figure.figsize':(8,5)})\n",
    "#sns.scatterplot(data=d, x=\"x_pca\", y=\"y_pca\", hue=\"maskbin\", alpha=0.9)\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "plt.scatter(mb_synth_stroke[:,0], mb_synth_stroke[:,1], label=\"1 (stroke) Synthetic\", alpha=0.5, s=20, marker=\".\", color=\"#FF6600\")\n",
    "plt.scatter(mb_synth_ns[:,0], mb_synth_ns[:,1], label=\"0 (negative) Synthetic\", alpha=0.7, s=20, marker=\".\", color=\"#0000FF\")\n",
    "plt.scatter(comp3[:,0], comp3[:,1], label=\"Synthetic MRI CTHEALTHY\", alpha=0.1, s=20, marker=\".\", color=\"#00AA00\")\n",
    "plt.scatter(c_stroke[0], c_stroke[1], label=\"\", alpha=1, s=100, marker=\"x\", color=\"#FF3300\")\n",
    "plt.scatter(c_ns[0], c_ns[1], label=\"\", alpha=1, s=100, marker=\"x\", color=\"#0000FF\")\n",
    "plt.scatter(c_h[0], c_h[1], label=\"\", alpha=1, s=100, marker=\"x\", color=\"#146d01\")\n",
    "#plt.scatter(comp4[:,0], comp4[:,1], label=\"Synthetic MRI CTFOSCAL\", alpha=0.8, s=5, color=\"#FF0000\")\n",
    "#plt.xticks(np.arange(-3, 20, 1))\n",
    "plt.legend(framealpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for k in range(d.shape[0]):\n",
    "    if(d.iloc[k][\"x_pca\"]>-2 and d.iloc[k][\"x_pca\"]<-1 and d.iloc[k][\"y_pca\"]>0 and d.iloc[k][\"y_pca\"]<2\n",
    "       and d.iloc[k][\"maskbin\"]==1):\n",
    "        print(d.iloc[k][\"maskbin\"], d.iloc[k][\"x_pca\"], d.iloc[k][\"y_pca\"], d.iloc[k][\"files\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "print(emb.shape)\n",
    "print(emb2.shape)\n",
    "um = umap.UMAP().fit_transform(emb[:,0,0,:])\n",
    "um2 = umap.UMAP().fit_transform(emb2[:,0,0,:])\n",
    "um3 = umap.UMAP().fit_transform(emb3[:,0,0,:])\n",
    "um4 = umap.UMAP().fit_transform(emb4[:,0,0,:])\n",
    "\n",
    "plt.scatter(um[:,0], um[:,1], label=\"Synthetic MRI\", alpha=0.5)\n",
    "plt.scatter(um2[:,0], um2[:,1], label=\"MRI_train\", alpha=0.5)\n",
    "plt.scatter(um3[:,0], um3[:,1], label=\"Synthetic MRI CTHEALTHY\", alpha=0.5)\n",
    "plt.scatter(um4[:,0], um4[:,1], label=\"Synthetic MRI CTFOSCAL\", alpha=0.5)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "d = pd.read_csv(\"data/xxx2.csv\")\n",
    "\n",
    "print(opt['trainA_image_names'][0])\n",
    "\n",
    "mb_list = []\n",
    "\n",
    "for i in range(len(opt['trainA_image_names'])):\n",
    "    #fname, comp[i,0], comp[i,1], maskbin\n",
    "    mb_list.append(list(d.loc[d[\"CT\"] == opt['trainA_image_names'][i]][\"maskbin\"])[0])\n",
    "\n",
    "d = {'files': opt[\"trainA_image_names\"], 'maskbin': mb_list, 'x_umap': um[:,0], 'y_umap': um[:,1], 'x_pca': comp[:,0], 'y_pca': comp[:,1]}\n",
    "d = pd.DataFrame(data=d)\n",
    "#d.head()\n",
    "#d.to_csv('pd-maskbin-'+str(ep)+'epoch.csv')\n",
    "\n",
    "#sns.scatterplot(data=d, x=\"x_pca\", y=\"y_pca\", hue=\"maskbin\")\n",
    "sns.set(rc={'figure.figsize':(8,5)})\n",
    "sns.scatterplot(data=d, x=\"x_umap\", y=\"y_umap\", hue=\"maskbin\", alpha=0.6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRI (trainB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "d = pd.read_csv(\"data/listas-mri-maskbin.csv\")\n",
    "print(\"shape: \",d.shape)\n",
    "print(len(opt['trainB_image_names']))\n",
    "print(opt['trainB_image_names'][0])\n",
    "\n",
    "mb_list = []\n",
    "\n",
    "mb_mri_ns = []\n",
    "mb_mri_stroke = []\n",
    "\n",
    "for i in range(len(opt['trainB_image_names'])):\n",
    "    #fname, comp[i,0], comp[i,1], maskbin\n",
    "    mb_list.append(list(d.loc[d[\"MRI\"] == opt['trainB_image_names'][i]][\"maskbin\"])[0])\n",
    "    if (list(d.loc[d[\"MRI\"] == opt['trainB_image_names'][i]][\"maskbin\"])[0]==0):\n",
    "        mb_mri_ns.append([comp2[i,0], comp2[i,1]])\n",
    "    else:\n",
    "        mb_mri_stroke.append([comp2[i,0], comp2[i,1]])\n",
    "\n",
    "mb_mri_ns = np.array(mb_mri_ns)\n",
    "mb_mri_stroke = np.array(mb_mri_stroke)\n",
    "\n",
    "d = {'files': opt[\"trainB_image_names\"], 'maskbin': mb_list, 'x_umap': um2[:,0], 'y_umap': um2[:,1], 'x_pca': comp2[:,0], 'y_pca': comp2[:,1]}\n",
    "d = pd.DataFrame(data=d)\n",
    "#d.head()\n",
    "#d.to_csv('pd-maskbin-'+str(ep)+'epoch.csv')\n",
    "\n",
    "#sns.scatterplot(data=d, x=\"x_pca\", y=\"y_pca\", hue=\"maskbin\")\n",
    "#sns.set(rc={'figure.figsize':(8,5)})\n",
    "#sns.scatterplot(data=d, x=\"x_pca\", y=\"y_pca\", hue=\"maskbin\", alpha=0.9)\n",
    "\n",
    "plt.scatter(mb_mri_ns[:,0], mb_mri_ns[:,1], label=\"0 (negative) MRI\", alpha=0.5)\n",
    "plt.scatter(mb_mri_stroke[:,0], mb_mri_stroke[:,1], label=\"1 (stroke) MRI\", alpha=0.5)\n",
    "plt.scatter(comp3[:,0], comp3[:,1], label=\"Synthetic MRI CTHEALTHY\", alpha=0.5)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "d = pd.read_csv(\"data/listas-mri-maskbin.csv\")\n",
    "print(\"shape: \",d.shape)\n",
    "print(len(opt['trainB_image_names']))\n",
    "print(opt['trainB_image_names'][0])\n",
    "\n",
    "mb_list = []\n",
    "\n",
    "for i in range(len(opt['trainB_image_names'])):\n",
    "    #fname, comp[i,0], comp[i,1], maskbin\n",
    "    mb_list.append(list(d.loc[d[\"MRI\"] == opt['trainB_image_names'][i]][\"maskbin\"])[0])\n",
    "\n",
    "d = {'files': opt[\"trainB_image_names\"], 'maskbin': mb_list, 'x_umap': um2[:,0], 'y_umap': um2[:,1], 'x_pca': comp2[:,0], 'y_pca': comp2[:,1]}\n",
    "d = pd.DataFrame(data=d)\n",
    "#d.head()\n",
    "#d.to_csv('pd-maskbin-'+str(ep)+'epoch.csv')\n",
    "\n",
    "#sns.scatterplot(data=d, x=\"x_pca\", y=\"y_pca\", hue=\"maskbin\")\n",
    "sns.set(rc={'figure.figsize':(8,5)})\n",
    "sns.scatterplot(data=d, x=\"x_umap\", y=\"y_umap\", hue=\"maskbin\", alpha=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CycleGAN_tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
